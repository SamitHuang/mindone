# model
model_version: llama-1B
batch_size: 64
checkpoint: "models/PixArt-Sigma-XL-2-256x256.ckpt"
vae_root: "models/vae"
text_encoder_root: "models/text_encoder"
tokenizer_root: "models/tokenizer"
scale_factor: 0.13025
enable_flash_attention: True
dtype: "bf16"

# training hyper-parameters
epochs: 100
scheduler: "constant"
start_learning_rate: 1.0e-4
optim: "adamw"
weight_decay: 0.1
loss_scaler_type: "static"
init_loss_scale: 1.0
gradient_accumulation_steps: 1
clip_grad: True
max_grad_norm: 1.0
ckpt_save_interval: 1
log_loss_interval: 1
recompute: True
text_drop_prob: 0.2
warmup_steps: 2000
